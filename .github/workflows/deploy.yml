name: deploy

on: [push, pull_request]
#  push:
#    tags:
#      - "v*.*.*"
#
#  workflow_dispatch:
#    inputs:
#      test_triton:
#        description: 'Manually test triton'
#        required: false
#        type: boolean
#        default: false

env:
  PIP_CACHE_DIR: /nvme/github-actions/pip-cache


jobs:
  test_functions:
    # if: startsWith(github.ref, 'refs/tags/') == true
    runs-on: [self-hosted, linux-a100]
    timeout-minutes: 4320 # 72hours
    environment: 'prod'
    container:
      image: nvcr.io/nvidia/tritonserver:22.12-py3
      options: "--gpus=all --ipc=host --user root -e PIP_CACHE_DIR=/root/.cache/pip"
      volumes:
        - /nvme/github-actions/pip-cache:/root/.cache/pip
    steps:
      - name: Setup systems
        run: |
          rm /etc/apt/sources.list.d/cuda*.list
          apt-get update && apt-get install -y --no-install-recommends rapidjson-dev libgoogle-glog-dev libgl1
          rm -rf /var/lib/apt/lists/*
      - name: Clone repository
        uses: actions/checkout@v2
      - name: Install pytorch
        run: |
          echo $(pwd)
          python3 -m pip cache dir
          python3 -m pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
          python3 -m pip install cmake packaging
      - name: Install lmdeploy from pypi
        if: startsWith(github.ref, 'refs/tags/') == true
        run: |
          python3 -m pip install lmdeploy[all]
          python3 -m pip install -r requirements/test.txt
          lmdeploy check_env
      - name: Install lmdeploy from source
        if: startsWith(github.ref, 'refs/tags/') != true
        run: |
          python3 -m pip install flash-attn --no-build-isolation
          python3 -m pip install -r requirements.txt -r requirements/test.txt
          cp -r /__w/build ./build
          cd build
          bash ../generate.sh
          make -j$(nproc) && make install
          cd ..
          python3 -m pip install .
          lmdeploy check_env
      - name: Test lmdeploy
        run: |
          echo "TODO"
      - name: Clear workfile
        if: always()
        run: |
          export repo_root=$(pwd)
          echo $repo_root
          rm -rf $repo_root/*

  test_triton:
    # if: inputs.test_triton == true || startsWith(github.ref, 'refs/tags/') == true
    runs-on: [self-hosted, linux-a100]
    timeout-minutes: 4320 # 72hours
    environment: 'prod'
    env:
      HF_MODEL: /nvme/qa_test_models/puyu-123b-w4a16-hf-v0.8.11
      WORKDIR: /nvme/qa_test_models/triton_workspace
    steps:
      - name: Clone repository
        uses: actions/checkout@v2
      - name: Create server container
        run: |
          export SERVER_ID=$(docker create \
            --rm \
            --gpus='"device=4,5,6,7"' \
            --ipc=host \
            --user root \
            --shm-size 16g \
            -p 33336:22 \
            -p 33337-33400:33337-33400 \
            --cap-add=SYS_PTRACE \
            --cap-add=SYS_ADMIN \
            --security-opt seccomp=unconfined \
            --name lmdeploy-ci-triton-server \
            --workdir /__w/lmdeploy/lmdeploy \
            --env PIP_CACHE_DIR=/root/.cache/pip \
            --env NCCL_LAUNCH_MODE=GROUP \
            --env http_proxy=${{secrets.PROXY}} \
            --env https_proxy=${{secrets.PROXY}} \
            -v $(pwd)/../../:/__w \
            -v ${HF_MODEL}:/root/workspace/hf_model \
            -v ${WORKDIR}:/root/workspace/workdir \
            -v ${PIP_CACHE_DIR}:/root/.cache/pip \
            --entrypoint "tail" \
            openmmlab/lmdeploy:latest "-f" "/dev/null" \
             )
          docker start $SERVER_ID
          echo "SERVER_ID=$SERVER_ID"
          echo "SERVER_ID=$SERVER_ID"  >> $GITHUB_ENV
      - name: Build lmdeploy inside server container
        run: |
          docker exec $SERVER_ID cp -r /__w/build /__w/lmdeploy/lmdeploy/build
          docker exec --workdir /__w/lmdeploy/lmdeploy/build $SERVER_ID  sh ../generate.sh
          docker exec --workdir /__w/lmdeploy/lmdeploy/build $SERVER_ID  make -j$(nproc)
          docker exec --workdir /__w/lmdeploy/lmdeploy/build $SERVER_ID  make install
      - name: Install lmdeploy inside server container
        run: |
          docker exec $SERVER_ID python3 -m pip install tritonclient[grpc]
          docker exec $SERVER_ID python3 -m pip install -r requirements/test.txt
          docker exec $SERVER_ID python3 -m pip install .
          # docker exec $SERVER_ID check_env
      - name: Convert to turbomind model
        run: |
          docker exec $SERVER_ID \
            lmdeploy convert \
            --model-name internlm-20b \
            --model-path /root/workspace/hf_model \
            --model-format awq \
            --tp 4 \
            --group-size 128 \
            --dst-path /root/workspace/workdir/tb_model

          docker exec $SERVER_ID chmod -R 777 /root/workspace/workdir/tb_model
      - name: Start triton server
        run: |
          docker exec --detach $SERVER_ID \
            tritonserver \
            --model-repository=/root/workspace/workdir/tb_model/model_repository \
            --allow-http=0 \
            --allow-grpc=1 \
            --grpc-port=33337 \
            --log-verbose=0 \
            --allow-metrics=1
      - name: Create client container
        run: |
          export CLIENT_ID=$(docker create \
            --gpus='"device=0"' \
            --rm \
            --workdir /__w/lmdeploy/lmdeploy \
            --network host \
            --env PIP_CACHE_DIR=/root/.cache/pip \
            --env http_proxy=${{secrets.PROXY}} \
            --env https_proxy=${{secrets.PROXY}} \
            -v $(pwd)/../../:/__w \
            -v ${WORKDIR}:/root/workspace/workdir \
            -v ${PIP_CACHE_DIR}:/root/.cache/pip \
            --entrypoint "tail" \
            openmmlab/lmdeploy:latest "-f" "/dev/null" \
             )
          docker start $CLIENT_ID
          echo "CLIENT_ID=$CLIENT_ID"
          echo "CLIENT_ID=$CLIENT_ID"  >> $GITHUB_ENV
      - name: Install lmdeploy inside client container
        run: |
          docker exec $SERVER_ID python3 -m pip install tritonclient[grpc]
          docker exec $SERVER_ID python3 -m pip install -r requirements/test.txt
          docker exec $CLIENT_ID python3 -m pip install .
          # docker exec $CLIENT_ID check_env
      - name: Test triton server
        run: |
          docker exec $CLIENT_ID python3 .github/scripts/test_triton.py
      - name: Clear workfile
        if: always()
        run: |
          docker exec $SERVER_ID rm -rf build lmdeploy/bin lmdeploy/lib lmdeploy.egg-info
          docker stop $SERVER_ID $CLIENT_ID
          export repo_root=$(pwd)
          echo $repo_root
          chmod -R 777 $repo_root
          rm -rf $repo_root/*
