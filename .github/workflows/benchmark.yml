name: perform

on:
  pull_request:
    paths-ignore:
      - "docs/**"
  push:
    paths-ignore:
      - "docs/**"

  workflow_dispatch:
    inputs:
      models:
        required: false
        description: 'Tested models. eg: ""'
        type: string
        default: ''

env:
  HOST_PIP_CACHE_DIR: /nvme/github-actions/pip-cache
  HOST_LOCALTIME: /usr/share/zoneinfo/Asia/Shanghai


jobs:
  test_performance:
    # if: startsWith(github.ref, 'refs/tags/') == true
    runs-on: [self-hosted, linux-a100]
    timeout-minutes: 4320 # 72hours
    environment: 'prod'
    env:
      REPORT_DIR: /nvme/github-actions/perform-reports
      TB_MODEL: /nvme/qa_test_models/perform-workspace/tb
      MODELS_ROOT: /nvme/qa_test_models
    container:
      image: nvcr.io/nvidia/tritonserver:22.12-py3
      options: "--gpus=all --ipc=host --user root -e PIP_CACHE_DIR=/root/.cache/pip -e CUDA_VISIBLE_DEVICES=5,6"
      volumes:
        - /nvme/github-actions/pip-cache:/root/.cache/pip
        - /nvme/github-actions/packages:/root/packages
        - /nvme/github-actions/resources:/root/resources
        - /nvme/qa_test_models:/nvme/qa_test_models
        - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro
    steps:
      - name: Setup systems
        run: |
          rm /etc/apt/sources.list.d/cuda*.list
          apt-get update && apt-get install -y --no-install-recommends rapidjson-dev \
              libgoogle-glog-dev libgl1
          rm -rf /var/lib/apt/lists
          export TIME_STAMP="$(date +'%Y%m%d-%H%M%S')"
          echo "TIME_STAMP=$TIME_STAMP" >> $GITHUB_ENV
      - name: Clone repository
        uses: actions/checkout@v2
      - name: Install pytorch
        run: |
          python3 -m pip cache dir
          python3 -m pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
      - name: Build lmdeploy
        run: |
          python3 -m pip install cmake
          python3 -m pip install -r requirements/build.txt
          # use cached build
          cp -r ../../build build
          cd build
          cmake .. \
              -DCMAKE_BUILD_TYPE=RelWithDebInfo \
              -DCMAKE_EXPORT_COMPILE_COMMANDS=1 \
              -DCMAKE_INSTALL_PREFIX=./install \
              -DBUILD_PY_FFI=ON \
              -DBUILD_MULTI_GPU=ON \
              -DCMAKE_CUDA_FLAGS="-lineinfo" \
              -DUSE_NVTX=ON \
              -DSM=80 \
              -DCMAKE_CUDA_ARCHITECTURES=80 \
              -DBUILD_TEST=OFF
          make -j$(nproc) && make install
      - name: Install lmdeploy from source
        run: |
          python3 -m pip install pynvml packaging nvidia-ml-py
          python3 -m pip install -r requirements.txt
          python3 -m pip install .
      - name: Convert llama2-7b-chat
        run: |
          echo "## llama2-7b-chat" >> $GITHUB_STEP_SUMMARY
          # convert model
          lmdeploy convert --model-name llama2 \
            --model-path ${MODELS_ROOT}/llama-2-7b-chat \
            --dst-path $TB_MODEL
      - name: Test latency of llama2-7b-chat
        run: |
          python3 benchmark/profile_generation.py \
            $TB_MODEL \
            --concurrency 1 \
            --prompt-tokens 32 \
            --completion-tokens 64 \
            --csv latency.csv
          echo "### Latency" >> $GITHUB_STEP_SUMMARY
          python3 .github/scripts/test_tools.py add_summary latency.csv
      - name: Test token throughput of llama2-7b-chat
        run: |
          python3 benchmark/profile_generation.py \
            $TB_MODEL \
            --concurrency 8 \
            --prompt-tokens 64 \
            --completion-tokens 512 \
            --csv token-throughput.csv
          echo "### Throughput" >> $GITHUB_STEP_SUMMARY
          echo "#### Token Throughput" >> $GITHUB_STEP_SUMMARY
          python3 .github/scripts/test_tools.py add_summary token-throughput.csv
      - name: Test req throughput of llama2-7b-chat
        run: |
          CUDA_VISIBLE_DEVICES=1 python3 benchmark/profile_throughput.py \
            --dataset /root/resources/ShareGPT_V3_unfiltered_cleaned_split.json \
            --model-path $TB_MODEL \
            --concurrency 8 \
            --num-prompts 100 \
            --csv req-throughput.csv

          echo "#### Req Throughput" >> $GITHUB_STEP_SUMMARY
          python3 .github/scripts/test_tools.py add_summary req-throughput.csv

          # collect reports
          mkdir -p $REPORT_DIR/$TIME_STAMP/llama2-7b-chat
          mv latency.csv token-throughput.csv req-throughput.csv $REPORT_DIR/$TIME_STAMP/llama2-7b-chat/

      - name: Convert llama2-13b-chat
        run: |
          echo "## llama2-13b-chat" >> $GITHUB_STEP_SUMMARY
          # convert model
          lmdeploy convert --model-name llama2 \
            --model-path ${MODELS_ROOT}/llama-2-13b-chat \
            --tp 2 \
            --dst-path $TB_MODEL
      - name: Test latency of llama2-13b-chat
        run: |
          python3 benchmark/profile_generation.py \
            $TB_MODEL \
            --tp 2 \
            --concurrency 1 \
            --prompt-tokens 32 \
            --completion-tokens 64 \
            --csv latency.csv
          echo "### Latency" >> $GITHUB_STEP_SUMMARY
          python3 .github/scripts/test_tools.py add_summary latency.csv
      - name: Test token throughput of llama2-13b-chat
        run: |
          python3 benchmark/profile_generation.py \
            $TB_MODEL \
            --tp 2 \
            --concurrency 8 \
            --prompt-tokens 64 \
            --completion-tokens 512 \
            --csv token-throughput.csv
          echo "### Throughput" >> $GITHUB_STEP_SUMMARY
          echo "#### Token Throughput" >> $GITHUB_STEP_SUMMARY
          python3 .github/scripts/test_tools.py add_summary token-throughput.csv
      - name: Test req throughput of llama2-13b-chat
        run: |
          CUDA_VISIBLE_DEVICES=1 python3 benchmark/profile_throughput.py \
            --model-path $TB_MODEL \
            --tp 2 \
            --concurrency 8 \
            --num-prompts 100 \
            --dataset /root/resources/ShareGPT_V3_unfiltered_cleaned_split.json \
            --csv req-throughput.csv

          echo "#### Req Throughput" >> $GITHUB_STEP_SUMMARY
          python3 .github/scripts/test_tools.py add_summary req-throughput.csv

          # collect reports
          mkdir -p $REPORT_DIR/$TIME_STAMP/llama2-13b-chat
          mv latency.csv token-throughput.csv req-throughput.csv $REPORT_DIR/$TIME_STAMP/llama2-13b-chat/
      - name: Clear workspace
        if: always()
        run: |
          export workdir=$(pwd)
          cd ..
          rm -rf $workdir
          mkdir $workdir
          chmod -R 777 $workdir
          rm -rf $TB_MODEL
